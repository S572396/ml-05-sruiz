{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "351d9909",
   "metadata": {},
   "source": [
    "# Lab 5: Project (Ensemble ML, Spiral)\n",
    "\n",
    "\n",
    "\n",
    "Author: Sandra Ruiz\n",
    "\n",
    "Date: April 10,2025 \n",
    "\n",
    "### Introduction\n",
    "\n",
    "Objective: \n",
    "In this project, we will work with a wine quality csv file to learn how to implement and evaluate more complex models when simpler techniques aren't enough. We will build on previous methods of training and testing the data to explore results. We will explore ensemble models, a powerful approach that combines multiple models to improve performance. Ensemble methods often outperform individual models by reducing overfitting and improving generalization.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da58a23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports Needed at the Top\n",
    "\n",
    "\n",
    "!!pip install pandas numpy matplotlib scikit-learn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    AdaBoostClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    BaggingClassifier,\n",
    "    VotingClassifier,\n",
    ")\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b85011",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Section 1. Load and Inspect the Data\n",
    "# Load the dataset (download from UCI and save in the same folder)\n",
    "df = pd.read_csv(\"winequality-red.csv\", sep=\";\")\n",
    "\n",
    "# Display structure and first few rows\n",
    "df.info()\n",
    "df.head()\n",
    "\n",
    "# The dataset includes 11 physicochemical input variables (features):\n",
    "# ---------------------------------------------------------------\n",
    "# - fixed acidity          mostly tartaric acid\n",
    "# - volatile acidity       mostly acetic acid (vinegar)\n",
    "# - citric acid            can add freshness and flavor\n",
    "# - residual sugar         remaining sugar after fermentation\n",
    "# - chlorides              salt content\n",
    "# - free sulfur dioxide    protects wine from microbes\n",
    "# - total sulfur dioxide   sum of free and bound forms\n",
    "# - density                related to sugar content\n",
    "# - pH                     acidity level (lower = more acidic)\n",
    "# - sulphates              antioxidant and microbial stabilizer\n",
    "# - alcohol                % alcohol by volume\n",
    "\n",
    "# The target variable is:\n",
    "# - quality (integer score from 0 to 10, rated by wine tasters)\n",
    "\n",
    "# We will simplify this target into three categories:\n",
    "#   - low (3–4), medium (5–6), high (7–8) to make classification feasible.\n",
    "#   - we will also make this numeric (we want both for clarity)\n",
    "# The dataset contains 1599 samples and 12 columns (11 features + target).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79e566aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1599 entries, 0 to 1598\n",
      "Data columns (total 1 columns):\n",
      " #   Column                                                                                                                                                                   Non-Null Count  Dtype \n",
      "---  ------                                                                                                                                                                   --------------  ----- \n",
      " 0   fixed acidity;\"volatile acidity\";\"citric acid\";\"residual sugar\";\"chlorides\";\"free sulfur dioxide\";\"total sulfur dioxide\";\"density\";\"pH\";\"sulphates\";\"alcohol\";\"quality\"  1599 non-null   object\n",
      "dtypes: object(1)\n",
      "memory usage: 12.6+ KB\n",
      "  fixed acidity;\"volatile acidity\";\"citric acid\";\"residual sugar\";\"chlorides\";\"free sulfur dioxide\";\"total sulfur dioxide\";\"density\";\"pH\";\"sulphates\";\"alcohol\";\"quality\"\n",
      "0   7.4;0.7;0;1.9;0.076;11;34;0.9978;3.51;0.56;9.4;5                                                                                                                     \n",
      "1   7.8;0.88;0;2.6;0.098;25;67;0.9968;3.2;0.68;9.8;5                                                                                                                     \n",
      "2  7.8;0.76;0.04;2.3;0.092;15;54;0.997;3.26;0.65;...                                                                                                                     \n",
      "3  11.2;0.28;0.56;1.9;0.075;17;60;0.998;3.16;0.58...                                                                                                                     \n",
      "4   7.4;0.7;0;1.9;0.076;11;34;0.9978;3.51;0.56;9.4;5                                                                                                                     \n"
     ]
    }
   ],
   "source": [
    "# Load spiral dataset\n",
    "spiral = pd.read_csv(r\"C:\\Users\\19564\\Desktop\\Ensemble.SRuiz\\ml-05-sruiz\\winequality-red.csv\")\n",
    "\n",
    "\n",
    "# Display basic information\n",
    "spiral.info()\n",
    "\n",
    "# Display first few rows\n",
    "print(spiral.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672e238c",
   "metadata": {},
   "source": [
    "### Section 2. Prepare the Data\n",
    "Includes cleaning, feature engineering, encoding, splitting, helper functions\n",
    "####  Define helper function that:\n",
    "\n",
    "#### Takes one input, the quality (which we will temporarily name q while in the function)\n",
    "#### And returns a string of the quality label (low, medium, high)\n",
    "#### This function will be used to create the quality_label column\n",
    "def quality_to_label(q):\n",
    "    if q <= 4:\n",
    "        return \"low\"\n",
    "    elif q <= 6:\n",
    "        return \"medium\"\n",
    "    else:\n",
    "        return \"high\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71939fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the apply() method on the quality column to create the new quality_label column\n",
    "df[\"quality_label\"] = df[\"quality\"].apply(quality_to_label)\n",
    "\n",
    "\n",
    "# Then, create a numeric column for modeling: 0 = low, 1 = medium, 2 = high\n",
    "def quality_to_number(q):\n",
    "    if q <= 4:\n",
    "        return 0\n",
    "    elif q <= 6:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "\n",
    "df[\"quality_numeric\"] = df[\"quality\"].apply(quality_to_number)\n",
    "\n",
    "Explain what we do and why as you proceed. \n",
    "#### creates a second helper that maps scores to numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce29737b",
   "metadata": {},
   "source": [
    "### Section 3. Feature Selection and Justification\n",
    "#### Define input features (X) and target (y)\n",
    "#### Features: all columns except 'quality' and 'quality_label' and 'quality_numberic' - drop these from the input array\n",
    "##### Target: quality_label (the new column we just created)\n",
    "X = df.drop(columns=[\"quality\", \"quality_label\", \"quality_numeric\"])  # Features\n",
    "y = df[\"quality_numeric\"]  # Target\n",
    "\n",
    "Explain / introduce your choices:\n",
    "\n",
    "We want to train only on physicochemical properties of the wine (like acidity, pH, alcohol content, etc. We’re treating this as a multi-class classification problem where we want to train a model to predict one of three categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5712e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Section 4. Split the Data into Train and Test\n",
    "# Train/test split (stratify to preserve class balance)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098f7106",
   "metadata": {},
   "source": [
    "### Section 5.  Evaluate Model Performance (Choose 2)\n",
    "\n",
    "Below is a list of  9 model variations. Choose two to focus on for your comparison. \n",
    "\n",
    "Option\tModel Name\tNotes\n",
    "1\tRandom Forest (100)\tA strong baseline model using 100 decision trees.\n",
    "\n",
    "2\tRandom Forest (200, max_depth=10)\tAdds more trees, but limits tree depth to reduce overfitting.\n",
    "\n",
    "3\tAdaBoost (100)\tBoosting method that focuses on correcting previous errors.\n",
    "\n",
    "4\tAdaBoost (200, lr=0.5)\tMore iterations and slower learning for better generalization.\n",
    "    \n",
    "5\tGradient Boosting (100)\tBoosting approach using gradient descent.\n",
    "\n",
    "6\tVoting (DT + SVM + NN)\tCombines diverse models by averaging their predictions.\n",
    "\n",
    "7\tVoting (RF + LR + KNN)\tAnother mix of different model types.\n",
    "\n",
    "8\tBagging (DT, 100)\tBuilds many trees in parallel on different samples.\n",
    "\n",
    "9\tMLP Classifier\tA basic neural network with one hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00841d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "# Assuming evaluate_model is defined to store results in a 'results' list/dictionary\n",
    "results = []\n",
    "\n",
    "\n",
    "# 1. Random Forest\n",
    "evaluate_model(\n",
    "    \"Random Forest (100)\",\n",
    "    RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    results,\n",
    ")\n",
    "\n",
    "# 3. AdaBoost \n",
    "evaluate_model(\n",
    "    \"AdaBoost (100)\",\n",
    "    AdaBoostClassifier(n_estimators=100, random_state=42),\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    results,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7fcda78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary of All Models:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest (100)</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.8875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AdaBoost (100)</td>\n",
       "      <td>0.834246</td>\n",
       "      <td>0.8250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model  train_accuracy  test_accuracy\n",
       "0  Random Forest (100)        1.000000         0.8875\n",
       "1       AdaBoost (100)        0.834246         0.8250"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary of All Models (sorted by Test Accuracy):\n",
      "                 model  train_accuracy  test_accuracy\n",
      "0  Random Forest (100)        1.000000         0.8875\n",
      "1       AdaBoost (100)        0.834246         0.8250\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset with semicolon separator and quoted headers\n",
    "df = pd.read_csv(\n",
    "    r\"C:\\Users\\19564\\Desktop\\Ensemble.SRuiz\\ml-05-sruiz\\winequality-red.csv\",\n",
    "    sep=';',\n",
    "    quotechar='\"'\n",
    ")\n",
    "\n",
    "#  Quality labeling (categorical)\n",
    "def quality_to_label(q):\n",
    "    if q <= 4:\n",
    "        return \"low\"\n",
    "    elif q <= 6:\n",
    "        return \"medium\"\n",
    "    else:\n",
    "        return \"high\"\n",
    "\n",
    "df[\"quality_label\"] = df[\"quality\"].apply(quality_to_label)\n",
    "\n",
    "#  Quality encoding (numeric)\n",
    "def quality_to_number(q):\n",
    "    if q <= 4:\n",
    "        return 0\n",
    "    elif q <= 6:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "df[\"quality_numeric\"] = df[\"quality\"].apply(quality_to_number)\n",
    "\n",
    "#  Feature/Target split\n",
    "X = df.drop(columns=[\"quality\", \"quality_label\", \"quality_numeric\"])  # Features\n",
    "y = df[\"quality_numeric\"]  # Target\n",
    "\n",
    "#  Train/Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "#  Evaluation function\n",
    "def evaluate_model(name, model, X_train, y_train, X_test, y_test, results):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    results.append({\n",
    "        \"model\": name,\n",
    "        \"train_accuracy\": accuracy_score(y_train, y_train_pred),\n",
    "        \"test_accuracy\": accuracy_score(y_test, y_test_pred),\n",
    "    })\n",
    "\n",
    "#  Model evaluation\n",
    "results = []\n",
    "\n",
    "# Model 1: Random Forest\n",
    "evaluate_model(\n",
    "    \"Random Forest (100)\",\n",
    "    RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    X_train, y_train, X_test, y_test, results\n",
    ")\n",
    "\n",
    "# Model 2: AdaBoost\n",
    "evaluate_model(\n",
    "    \"AdaBoost (100)\",\n",
    "    AdaBoostClassifier(n_estimators=100, random_state=42),\n",
    "    X_train, y_train, X_test, y_test, results\n",
    ")\n",
    "\n",
    "#  Show results with gap calculation\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df[\"gap\"] = results_df[\"train_accuracy\"] - results_df[\"test_accuracy\"]\n",
    "results_df = results_df.sort_values(by=\"test_accuracy\", ascending=False)\n",
    "\n",
    "### Section 6. Compare Results \n",
    "# Create a table of results \n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\nSummary of All Models:\")\n",
    "display(results_df)\n",
    "\n",
    "####Recommendation: See if you can add gap calculations to your results and sort the table by test accuracy to find the best models more efficiently.\n",
    "\n",
    "\n",
    "# Display results\n",
    "print(\"\\nSummary of All Models (sorted by Test Accuracy):\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b19ab1",
   "metadata": {},
   "source": [
    "###\n",
    " Section 7. Conclusions and Insights\n",
    "Using both your results and the results from others, which options are performing well and why do you think so?   Discuss the types of models and why you think some seem to be more helpful. List the next steps you'd like to try if you were in a competition to build the best predictor.  \n",
    "\n",
    "Results:\n",
    "\n",
    "Two models were tested to see which one could best predict red wine quality. Random Forest had the best test score at 88.75%, but it did get an excellent score on the training set of (100%), which means it might just be memorizing instead of really learning. AdaBoost did a bit worse with 82.5% on the test and 83.4% on training, but that shows it's more balanced and probably better at working with new data. Random Forest is strong because it uses lots of decision trees together, and AdaBoost keeps learning from its mistakes to get better. If I had more time, I’d try changing model settings, test new models, and add new features to improve predictions. In the end, AdaBoost feels like the safer choice, and Random Forest is powerful but can be risky. For a competition I would try more models and cross validation with K-fold.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
